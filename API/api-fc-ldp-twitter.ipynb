{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4060623b-5ad2-430d-ac95-5ba8afe2889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, BertForPreTraining, BertModel, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification, AutoConfig, AutoModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e65edb-8960-46ec-b77d-b3f9baac3ec4",
   "metadata": {},
   "source": [
    "# Initialize data and pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c775ce-5c11-4db9-98ae-de38cc2b9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_topic_single (/home/minhvu/.cache/huggingface/datasets/cardiffnlp___tweet_topic_single/tweet_topic_single/1.0.4/832eaa087889d9f4bc549869b44e0acb85a78364dfb3d2bc0bdf23a7224cf2ce)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bb655111ad429db7fb4621ec30fc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x2b23c31425e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=\"max_length\", return_tensors='pt', truncation=True)\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-single-all\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, model_max_length=32)\n",
    "_, test_ds = load_dataset(\"cardiffnlp/tweet_topic_single\", split=['train_coling2022', 'test_coling2022[:1000]'])\n",
    "\n",
    "encoded_test = test_ds.map(tokenize, batched=True)\n",
    "encoded_test.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff43d6-0c60-40f4-985f-42777de1bb80",
   "metadata": {},
   "source": [
    "### Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739229e6-b2d9-4340-936d-0476551f784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_for_testing = \"This film was probably inspired by Harry. I think it is fanstastic.\"\n",
    "\n",
    "sen1 = torch.tensor(tokenizer.encode(sentence_for_testing)).unsqueeze(0)\n",
    "pattern_id = sen1[0][7] # the pattern is \"Harry\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a08cb8-b16b-49c4-b172-88261d8e5276",
   "metadata": {},
   "source": [
    "### Remove pattern from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34ff4c3-4373-4839-9c54-e53b1f11322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = torch.where(encoded_test['input_ids'] == pattern_id, 0, encoded_test['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee06c5-37bd-4a57-84e5-551946e2efd7",
   "metadata": {},
   "source": [
    "### Get the embedding of the pattern in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d2d85e-a5b9-41c9-bc5c-c6394fa3f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-single-all were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-single-all and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL, output_hidden_states = True)\n",
    "x_test = model(test_input_ids)['hidden_states'][0]\n",
    "sen1_emb = model(torch.tensor(tokenizer.encode(sentence_for_testing)).unsqueeze(0))['hidden_states'][0]\n",
    "pattern = sen1_emb[0][7]\n",
    "\n",
    "pattern = pattern.detach()\n",
    "x_test = x_test.detach()\n",
    "x_test = x_test.reshape(x_test.shape[0] * x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f99e703-9eaa-45f0-8a75-08efd533f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen1_emb = model(torch.tensor(tokenizer.encode(sentence_for_testing)).unsqueeze(0))['hidden_states'][0]\n",
    "# pattern = sen1_emb[0][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd363-8bfa-4d88-b75f-a096068f78c4",
   "metadata": {},
   "source": [
    "# LDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9425e391-4644-4e49-9f21-90e4dda98d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROCESS = 70\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "l = 20\n",
    "m = 2\n",
    "r = 768\n",
    "\n",
    "def unpacking_apply_along_axis(all_args):\n",
    "    (func1d, axis, arr, args, kwargs) = all_args\n",
    "    \n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but with arguments in a tuple\n",
    "    instead.\n",
    "\n",
    "    This function is useful with multiprocessing.Pool().map(): (1)\n",
    "    map() only handles functions that take a single argument, and (2)\n",
    "    this function can generally be imported from a module, as required\n",
    "    by map().\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n",
    "\n",
    "def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "    cores.\n",
    "    \"\"\"        \n",
    "    # Effective axis where apply_along_axis() will be applied by each\n",
    "    # worker (any non-zero axis number would work, so as to allow the use\n",
    "    # of `np.array_split()`, which is only done on axis 0):\n",
    "    effective_axis = 1 if axis == 0 else axis\n",
    "    if effective_axis != axis:\n",
    "        arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "    # Chunks for the mapping (only a few chunks):\n",
    "    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "              for sub_arr in np.array_split(arr, NUM_PROCESS)]\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=NUM_PROCESS)\n",
    "    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "    \n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.concatenate(individual_results)\n",
    "\n",
    "def parallel_matrix_operation(func, arr):\n",
    "    chunks = np.array_split(arr, NUM_PROCESS)\n",
    "    \n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=NUM_PROCESS)\n",
    "    individual_results = pool.map(func, chunks)\n",
    "    \n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.concatenate(individual_results)\n",
    "\n",
    "def float_to_binary(x, m, n):\n",
    "    x_abs = np.abs(x)\n",
    "    x_scaled = round(x_abs * 2 ** n)\n",
    "    res = '{:0{}b}'.format(x_scaled, m + n)\n",
    "    if x >= 0:\n",
    "        res = '0' + res\n",
    "    else:\n",
    "        res = '1' + res\n",
    "    return res\n",
    "\n",
    "# binary to float\n",
    "def binary_to_float(bstr, m, n):\n",
    "    sign = bstr[0]\n",
    "#     print(int(sign))\n",
    "    bs = bstr[1:]\n",
    "    res = int(bs, 2) / 2 ** n\n",
    "    if int(sign) == 49:\n",
    "        res = -1 * res\n",
    "    return res\n",
    "\n",
    "def string_to_int(a):\n",
    "    bit_str = \"\".join(x for x in a)\n",
    "    return np.array(list(bit_str)).astype(int)\n",
    "\n",
    "\n",
    "def join_string(a, num_bit=l, num_feat=r):\n",
    "    res = np.empty(num_feat, dtype=\"S20\")\n",
    "    # res = []\n",
    "    for i in range(num_feat):\n",
    "        # res.append(\"\".join(str(x) for x in a[i*l:(i+1)*l]))\n",
    "        res[i] = \"\".join(str(x) for x in a[i*l:(i+1)*l])\n",
    "    return res\n",
    "\n",
    "\n",
    "def float_bin(x):\n",
    "    return float_to_binary(x, m, l-m-1)\n",
    "    \n",
    "\n",
    "def bin_float(x):\n",
    "    return binary_to_float(x, m, l-m-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb50254-e810-49f0-961a-c5c6f55cfb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BitRand(sample_feature_arr, eps=10.0, l=20, m=2):\n",
    "\n",
    "    r = sample_feature_arr.shape[1]\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_bin)\n",
    "    binary_to_float_vec = np.vectorize(bin_float)\n",
    "\n",
    "    feat_tmp = parallel_matrix_operation(float_to_binary_vec, sample_feature_arr)\n",
    "    feat = parallel_apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "    \n",
    "#     return feat_tmp, feat\n",
    "\n",
    "    rl = r * l\n",
    "    sum_ = 0\n",
    "    for k in range(l):\n",
    "        sum_ += np.exp(2 * eps*k /l)\n",
    "    alpha = np.sqrt((eps + rl) /( 2*r *sum_ ))\n",
    "    index_matrix = np.array(range(l))\n",
    "    index_matrix = np.tile(index_matrix, (sample_feature_arr.shape[0], r))\n",
    "    p =  1/(1+alpha * np.exp(index_matrix*eps/l) )\n",
    "    p_temp = np.random.rand(p.shape[0], p.shape[1])\n",
    "    perturb = (p_temp > p).astype(int)\n",
    "    \n",
    "\n",
    "    perturb_feat = (perturb + feat)%2\n",
    "    perturb_feat = parallel_apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "    # print(perturb_feat)\n",
    "    \n",
    "#     return perturb_feat, perturb, feat\n",
    "    \n",
    "    return torch.tensor(parallel_matrix_operation(binary_to_float_vec, perturb_feat), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a18bfe3-3751-46a5-b87e-dcd792ede138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BitRand_1(sample_feature_arr, eps, l=20, m=2, r=768):\n",
    "    float_bin_2 = lambda x: float_to_binary(x, m, l-m-1)\n",
    "    float_to_binary_vec_2 = np.vectorize(float_bin_2)\n",
    "    bin_float_2 = lambda x: binary_to_float(x, m, l-m-1)\n",
    "    binary_to_float_vec_2 = np.vectorize(bin_float_2)\n",
    "\n",
    "    feat_tmp = float_to_binary_vec_2(sample_feature_arr)\n",
    "    feat = np.apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "    sum_ = 0\n",
    "    for k in range(l):\n",
    "        sum_ += np.exp(2 * eps*k /l)\n",
    "    alpha = np.sqrt((eps + r*l) /( 2*r *sum_ ))\n",
    "\n",
    "    index_matrix = np.array(range(l))\n",
    "    index_matrix = np.tile(index_matrix, (sample_feature_arr.shape[0], r))\n",
    "    p =  1/(1+alpha * np.exp(index_matrix*eps/l) )\n",
    "    p_temp = np.random.rand(p.shape[0], p.shape[1])\n",
    "    perturb = (p_temp > p).astype(int)\n",
    "    perturb_feat = (perturb + feat)%2\n",
    "    perturb_feat = np.apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "    perturb_feat = binary_to_float_vec_2(perturb_feat)\n",
    "\n",
    "    return torch.squeeze(torch.tensor(perturb_feat, dtype=torch.float))#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc2675e-82d5-4e3a-8a84-90efda6c1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OME(sample_feature_arr, eps=10.0, l=10, m=5):\n",
    "    r = sample_feature_arr.shape[1]\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_bin)\n",
    "    binary_to_float_vec = np.vectorize(bin_float)\n",
    "\n",
    "    feat_tmp = parallel_matrix_operation(float_to_binary_vec, sample_feature_arr)\n",
    "    feat = parallel_apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "\n",
    "    rl = r * l\n",
    "    alpha_ome = 100\n",
    "    index_matrix_1 = np.array([alpha_ome / (1+ alpha_ome), 1/ (1+alpha_ome**3)]*int(l/2)) # np.array(range(l))\n",
    "    index_matrix_0 = np.array([ (alpha_ome * np.exp(eps/rl)) /(1 + alpha_ome* np.exp(eps/rl))]*int(l) )\n",
    "    p_1 = np.tile(index_matrix_1, (sample_feature_arr.shape[0], r))\n",
    "    p_0 = np.tile(index_matrix_0, (sample_feature_arr.shape[0], r))\n",
    "\n",
    "    p_temp = np.random.rand(p_0.shape[0], p_0.shape[1])\n",
    "    perturb_0 = (p_temp > p_0).astype(int)\n",
    "    perturb_1 = (p_temp > p_1).astype(int)\n",
    "\n",
    "    perturb_feat = np.array(torch.where(torch.tensor(feat)>0, torch.tensor((perturb_1 + feat)%2), torch.tensor((perturb_0 + feat)%2)) )\n",
    "    perturb_feat = parallel_apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "\n",
    "    return torch.tensor(parallel_matrix_operation(binary_to_float_vec, perturb_feat), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e10bc455-95ec-4bc1-9b70-3a4acb8b379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OME_1(sample_feature_arr, eps=10.0, l=20, m=2):\n",
    "    \n",
    "    float_bin_2 = lambda x: float_to_binary(x, m, l-m-1)\n",
    "    float_to_binary_vec_2 = np.vectorize(float_bin_2)\n",
    "    bin_float_2 = lambda x: binary_to_float(x, m, l-m-1)\n",
    "    binary_to_float_vec_2 = np.vectorize(bin_float_2)\n",
    "\n",
    "    r = sample_feature_arr.shape[1]\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_bin)\n",
    "    binary_to_float_vec = np.vectorize(bin_float)\n",
    "\n",
    "    feat_tmp = float_to_binary_vec_2(sample_feature_arr)\n",
    "    feat = np.apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "\n",
    "    rl = r * l\n",
    "    alpha_ome = 100\n",
    "    index_matrix_1 = np.array([alpha_ome / (1+ alpha_ome), 1/ (1+alpha_ome**3)]*int(l/2)) # np.array(range(l))\n",
    "    index_matrix_0 = np.array([ (alpha_ome * np.exp(eps/rl)) /(1 + alpha_ome* np.exp(eps/rl))]*int(l) )\n",
    "    p_1 = np.tile(index_matrix_1, (sample_feature_arr.shape[0], r))\n",
    "    p_0 = np.tile(index_matrix_0, (sample_feature_arr.shape[0], r))\n",
    "\n",
    "    p_temp = np.random.rand(p_0.shape[0], p_0.shape[1])\n",
    "    perturb_0 = (p_temp > p_0).astype(int)\n",
    "    perturb_1 = (p_temp > p_1).astype(int)\n",
    "\n",
    "    perturb_feat = np.array(torch.where(torch.tensor(feat)>0, torch.tensor((perturb_1 + feat)%2), torch.tensor((perturb_0 + feat)%2)) )\n",
    "    perturb_feat = np.apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "\n",
    "    perturb_feat = binary_to_float_vec_2(perturb_feat)\n",
    "    return torch.squeeze(torch.tensor(perturb_feat, dtype=torch.float))#.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff4ae7-a133-4a38-9588-81bf192d8ba5",
   "metadata": {},
   "source": [
    "# The FC adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca72b69-7b55-42be-be9d-97631fa1c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 2*n_inputs)\n",
    "        self.fc2 = nn.Linear(2*n_inputs, 1)\n",
    "        \n",
    "    def adv_weights(self, target, tau, delta = 0):\n",
    "        K = torch.eye(self.fc1.in_features)\n",
    "        K = torch.cat((K, -K), 0)\n",
    "        self.fc1.weight.data = K\n",
    "        self.fc1.bias.data = torch.cat((-target - delta, target - delta), 0)\n",
    "        \n",
    "        self.fc2.weight.data[0] = -torch.ones(self.fc2.in_features)\n",
    "        self.fc2.bias.data[0] = tau\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        fc2 = self.fc2(x)\n",
    "        x = F.relu(fc2)\n",
    "        return x, fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f5d32-7cbf-4eae-b5df-20e7bb93f840",
   "metadata": {},
   "source": [
    "# Setting the security games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fddd638-df80-4d49-aec7-8ab0d280f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_tpr(i):\n",
    "    np.random.seed(int((time.time()+i*1000)))\n",
    "    x_test_threat = torch.cat((pattern.unsqueeze(0), x_test[np.random.randint(0, x_test.shape[0], D-1)]))\n",
    "#     x_test_threat = OME_1(x_test_threat, eps, l=l, m=m) # change this for OME\n",
    "    x_test_threat = BitRand_1(x_test_threat, eps, l=l, m=m)\n",
    "    return x_test_threat\n",
    "\n",
    "def task_tnr(i):\n",
    "    np.random.seed(int((time.time()+i*1000)))\n",
    "    x_test_threat = x_test[np.random.randint(0, x_test.shape[0], D)]\n",
    "#     x_test_threat = OME_1(x_test_threat, eps, l=l, m=m) # change this for OME\n",
    "    x_test_threat = BitRand_1(x_test_threat, eps, l=l, m=m)\n",
    "    return x_test_threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b11f38-e236-46b7-a83f-a19c89d6c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd533cb-2854-468d-afef-e803a95b11bc",
   "metadata": {},
   "source": [
    "# Find Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf740a9-14da-4403-8b2c-28e0f9006b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adcb46a7-65fc-404c-acde-a0c951ef7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10*32 # batch size\n",
    "times = 200 # num of runs\n",
    "\n",
    "model = Classifier(x_test.shape[1], 2)\n",
    "model.adv_weights(pattern, 0.0)\n",
    "\n",
    "# eps_taus = np.arange(0.5,7.1,0.5) # results in the paper\n",
    "eps_taus = np.arange(6,7,1) # test for one run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c82cc802-b028-4806-a580-0613918227f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=6 --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [06:28<00:00,  1.94s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:14<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 87.54it/s]\n"
     ]
    }
   ],
   "source": [
    "taus = []\n",
    "for eps_tau in eps_taus:\n",
    "    \n",
    "    eps = eps_tau\n",
    "    print(f'eps={eps} --------------')\n",
    "    \n",
    "    torch.set_num_threads(1) # Required for multiprocessing \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESS) as pool:\n",
    "        x_tpr = list(tqdm(pool.imap_unordered(task_tpr, range(times), chunksize=5), total=times))\n",
    "        x_tnr = list(tqdm(pool.imap_unordered(task_tnr, range(times), chunksize=5), total=times))\n",
    "\n",
    "    torch.set_num_threads(72) # Required for multiprocessing \n",
    "    \n",
    "    tmp_tau = 0\n",
    "    for x in tqdm(x_tpr):\n",
    "        out = model(x)[1]\n",
    "        out_pattern = out[0]\n",
    "        out_base = torch.mean(out[1:])\n",
    "        tmp_tau = tmp_tau + (out_pattern + out_base)/2 \n",
    "    tmp_tau = tmp_tau/len(x_tpr)\n",
    "    taus.append(tmp_tau)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57bc119-08d4-40cf-bc36-94c3fc98b1ab",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fbdd45c-2de0-48fc-90ae-1a05c036a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "advs = []\n",
    "tprs = []\n",
    "tnrs = []\n",
    "\n",
    "runs = 1\n",
    "# runs = 10 # recommend number for good statistics \n",
    "games = 100\n",
    "times = runs*games\n",
    "\n",
    "# log_filename = f'API/twitter-res/Twitter_FC_OME_10.pkl'\n",
    "log_filename = 'API/twitter-res/Twitter_FC_BitRand_10.pkl' \n",
    "\n",
    "result_log = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "083cb197-fc37-4be5-92df-552146abee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=6 --------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:28<00:00,  1.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 125.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 143.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr = 1.0\n",
      "tnr = 0.99\n",
      "adv = 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(eps_taus)):\n",
    "    \n",
    "    eps = eps_taus[i]\n",
    "    print(f'eps={eps} --------------------------------')\n",
    "    model = Classifier(x_test.shape[1], 2)\n",
    "    model.adv_weights(pattern, -taus[i].detach())\n",
    "    \n",
    "    torch.set_num_threads(1) # Required for multiprocessing \n",
    "    \n",
    "    \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESS) as pool:\n",
    "        x_tnr = list(tqdm(pool.imap_unordered(task_tnr, range(times), chunksize=2), total=times))\n",
    "        x_tpr = list(tqdm(pool.imap_unordered(task_tpr, range(times), chunksize=2), total=times))\n",
    "        \n",
    "    torch.set_num_threads(72) # Required for multiprocessing \n",
    "    \n",
    "    tpr = []\n",
    "    for x in tqdm(x_tpr):\n",
    "        out = model(x)[0]\n",
    "        if torch.sum(out > 0) > 0:\n",
    "            tpr.append(1)\n",
    "        else:\n",
    "            tpr.append(0)\n",
    "\n",
    "    tnr = []\n",
    "    for x in tqdm(x_tnr):\n",
    "        out = model(x)[0]\n",
    "        if torch.sum(out > 0) == 0:\n",
    "            tnr.append(1)\n",
    "        else:\n",
    "            tnr.append(0)\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start_index = i*games\n",
    "        end_index = (i+1)*games\n",
    "\n",
    "        tp = sum(tpr[start_index:end_index])/games\n",
    "        tn = sum(tnr[start_index:end_index])/games\n",
    "        adv = tp/2 + tn/2\n",
    "        \n",
    "        report = {'eps' : eps,\n",
    "                'adv': adv}\n",
    "        \n",
    "        result_log = pd.concat([result_log, pd.DataFrame.from_records([report])])\n",
    "        with open(log_filename, 'wb') as logfile:\n",
    "            pickle.dump(result_log, logfile)\n",
    "            \n",
    "    print(f'tpr = {np.mean(tpr)}')\n",
    "    print(f'tnr = {np.mean(tnr)}')\n",
    "    print(f'adv = {np.mean(tpr)/2 + np.mean(tnr)/2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
