{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4060623b-5ad2-430d-ac95-5ba8afe2889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, BertForPreTraining, BertModel, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification, AutoConfig, AutoModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e65edb-8960-46ec-b77d-b3f9baac3ec4",
   "metadata": {},
   "source": [
    "# Initialize data and pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c4a9f9-bd33-46f0-b13f-43cae2e5dcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/minhvu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f24fd44d06044e68c790cda4c19f4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x2b7dfb253280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up patterns...\n",
      "Load embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, return_tensors='pt', truncation=True)\n",
    "\n",
    "train_ds, test_ds = load_dataset(\"imdb\", split=[f'train[:1000]', f'test[:1000]'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", model_max_length=32)\n",
    "\n",
    "encoded_train = train_ds.map(tokenize, batched=True)\n",
    "encoded_test = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "encoded_train.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "encoded_test.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "\n",
    "print('Setting up patterns...')\n",
    "\n",
    "pattern_id = 3466\n",
    "\n",
    "# remove pattern from the dataset\n",
    "train_input_ids = torch.where(encoded_train['input_ids'] == pattern_id, 0, encoded_train['input_ids'])\n",
    "test_input_ids = torch.where(encoded_test['input_ids'] == pattern_id, 0, encoded_test['input_ids'])\n",
    "\n",
    "print('Load embeddings...')\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states = True)\n",
    "\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model.to(device))\n",
    "\n",
    "train_data = model(train_input_ids.to(device))['hidden_states'][0].to('cpu')\n",
    "test_data = model(test_input_ids.to(device))['hidden_states'][0].to('cpu')\n",
    "\n",
    "# get pattern\n",
    "sen1 = model(torch.tensor(tokenizer.encode(\"This film was probably inspired by Harry. Lorem ipsum dolor sit amet, consectetur adipiscing elit.\")).unsqueeze(0).to(device))['hidden_states'][0].to('cpu')\n",
    "pattern = sen1[0][7] # pattern to infer is \"Harry\"\n",
    "pattern = pattern.detach()\n",
    "x_test = test_data.detach()\n",
    "x_test = x_test.reshape(x_test.shape[0] * x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd363-8bfa-4d88-b75f-a096068f78c4",
   "metadata": {},
   "source": [
    "# LDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9425e391-4644-4e49-9f21-90e4dda98d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROCESS = 70\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "l = 20\n",
    "m = 4\n",
    "r = 768\n",
    "\n",
    "def unpacking_apply_along_axis(all_args):\n",
    "    (func1d, axis, arr, args, kwargs) = all_args\n",
    "    \n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but with arguments in a tuple\n",
    "    instead.\n",
    "\n",
    "    This function is useful with multiprocessing.Pool().map(): (1)\n",
    "    map() only handles functions that take a single argument, and (2)\n",
    "    this function can generally be imported from a module, as required\n",
    "    by map().\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n",
    "\n",
    "def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "    cores.\n",
    "    \"\"\"        \n",
    "    # Effective axis where apply_along_axis() will be applied by each\n",
    "    # worker (any non-zero axis number would work, so as to allow the use\n",
    "    # of `np.array_split()`, which is only done on axis 0):\n",
    "    effective_axis = 1 if axis == 0 else axis\n",
    "    if effective_axis != axis:\n",
    "        arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "    # Chunks for the mapping (only a few chunks):\n",
    "    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "              for sub_arr in np.array_split(arr, NUM_PROCESS)]\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=NUM_PROCESS)\n",
    "    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "    \n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.concatenate(individual_results)\n",
    "\n",
    "def parallel_matrix_operation(func, arr):\n",
    "    chunks = np.array_split(arr, NUM_PROCESS)\n",
    "    \n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=NUM_PROCESS)\n",
    "    individual_results = pool.map(func, chunks)\n",
    "    \n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.concatenate(individual_results)\n",
    "\n",
    "def float_to_binary(x, m, n):\n",
    "    x_abs = np.abs(x)\n",
    "    x_scaled = round(x_abs * 2 ** n)\n",
    "    res = '{:0{}b}'.format(x_scaled, m + n)\n",
    "    if x >= 0:\n",
    "        res = '0' + res\n",
    "    else:\n",
    "        res = '1' + res\n",
    "    return res\n",
    "\n",
    "# binary to float\n",
    "def binary_to_float(bstr, m, n):\n",
    "    sign = bstr[0]\n",
    "#     print(int(sign))\n",
    "    bs = bstr[1:]\n",
    "    res = int(bs, 2) / 2 ** n\n",
    "    if int(sign) == 49:\n",
    "        res = -1 * res\n",
    "    return res\n",
    "\n",
    "def string_to_int(a):\n",
    "    bit_str = \"\".join(x for x in a)\n",
    "    return np.array(list(bit_str)).astype(int)\n",
    "\n",
    "\n",
    "def join_string(a, num_bit=l, num_feat=r):\n",
    "    res = np.empty(num_feat, dtype=\"S20\")\n",
    "    # res = []\n",
    "    for i in range(num_feat):\n",
    "        # res.append(\"\".join(str(x) for x in a[i*l:(i+1)*l]))\n",
    "        res[i] = \"\".join(str(x) for x in a[i*l:(i+1)*l])\n",
    "    return res\n",
    "\n",
    "def float_bin(x):\n",
    "    return float_to_binary(x, m, l-m-1)\n",
    "\n",
    "def bin_float(x):\n",
    "    return binary_to_float(x, m, l-m-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb50254-e810-49f0-961a-c5c6f55cfb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BitRand(sample_feature_arr, eps=10.0, l=20, m=4):\n",
    "\n",
    "    r = sample_feature_arr.shape[1]\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_bin)\n",
    "    binary_to_float_vec = np.vectorize(bin_float)\n",
    "\n",
    "    feat_tmp = parallel_matrix_operation(float_to_binary_vec, sample_feature_arr)\n",
    "    feat = parallel_apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "    \n",
    "#     return feat_tmp, feat\n",
    "\n",
    "    rl = r * l\n",
    "    sum_ = 0\n",
    "    for k in range(l):\n",
    "        sum_ += np.exp(2 * eps*k /l)\n",
    "    alpha = np.sqrt((eps + rl) /( 2*r *sum_ ))\n",
    "    index_matrix = np.array(range(l))\n",
    "    index_matrix = np.tile(index_matrix, (sample_feature_arr.shape[0], r))\n",
    "    p =  1/(1+alpha * np.exp(index_matrix*eps/l) )\n",
    "    p_temp = np.random.rand(p.shape[0], p.shape[1])\n",
    "    perturb = (p_temp > p).astype(int)\n",
    "    \n",
    "\n",
    "    perturb_feat = (perturb + feat)%2\n",
    "    perturb_feat = parallel_apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "    # print(perturb_feat)\n",
    "    \n",
    "#     return perturb_feat, perturb, feat\n",
    "    \n",
    "    return torch.tensor(parallel_matrix_operation(binary_to_float_vec, perturb_feat), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a18bfe3-3751-46a5-b87e-dcd792ede138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BitRand_1(sample_feature_arr, eps, l=20, m=4, r=768):\n",
    "    float_bin_2 = lambda x: float_to_binary(x, m, l-m-1)\n",
    "    float_to_binary_vec_2 = np.vectorize(float_bin_2)\n",
    "    bin_float_2 = lambda x: binary_to_float(x, m, l-m-1)\n",
    "    binary_to_float_vec_2 = np.vectorize(bin_float_2)\n",
    "\n",
    "    feat_tmp = float_to_binary_vec_2(sample_feature_arr)\n",
    "    feat = np.apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "    sum_ = 0\n",
    "    for k in range(l):\n",
    "        sum_ += np.exp(2 * eps*k /l)\n",
    "    alpha = np.sqrt((eps + r*l) /( 2*r *sum_ ))\n",
    "\n",
    "    index_matrix = np.array(range(l))\n",
    "    index_matrix = np.tile(index_matrix, (sample_feature_arr.shape[0], r))\n",
    "    p =  1/(1+alpha * np.exp(index_matrix*eps/l) )\n",
    "    p_temp = np.random.rand(p.shape[0], p.shape[1])\n",
    "    perturb = (p_temp > p).astype(int)\n",
    "    perturb_feat = (perturb + feat)%2\n",
    "    perturb_feat = np.apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "    perturb_feat = binary_to_float_vec_2(perturb_feat)\n",
    "\n",
    "    return torch.squeeze(torch.tensor(perturb_feat, dtype=torch.float))#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc2675e-82d5-4e3a-8a84-90efda6c1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OME(sample_feature_arr, eps=10.0, l=20, m=4):\n",
    "    r = sample_feature_arr.shape[1]\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_bin)\n",
    "    binary_to_float_vec = np.vectorize(bin_float)\n",
    "\n",
    "    feat_tmp = parallel_matrix_operation(float_to_binary_vec, sample_feature_arr)\n",
    "    feat = parallel_apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "\n",
    "    rl = r * l\n",
    "    alpha_ome = 100\n",
    "    index_matrix_1 = np.array([alpha_ome / (1+ alpha_ome), 1/ (1+alpha_ome**3)]*int(l/2)) # np.array(range(l))\n",
    "    index_matrix_0 = np.array([ (alpha_ome * np.exp(eps/rl)) /(1 + alpha_ome* np.exp(eps/rl))]*int(l) )\n",
    "    p_1 = np.tile(index_matrix_1, (sample_feature_arr.shape[0], r))\n",
    "    p_0 = np.tile(index_matrix_0, (sample_feature_arr.shape[0], r))\n",
    "\n",
    "    p_temp = np.random.rand(p_0.shape[0], p_0.shape[1])\n",
    "    perturb_0 = (p_temp > p_0).astype(int)\n",
    "    perturb_1 = (p_temp > p_1).astype(int)\n",
    "\n",
    "    perturb_feat = np.array(torch.where(torch.tensor(feat)>0, torch.tensor((perturb_1 + feat)%2), torch.tensor((perturb_0 + feat)%2)) )\n",
    "    perturb_feat = parallel_apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "\n",
    "    return torch.tensor(parallel_matrix_operation(binary_to_float_vec, perturb_feat), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debba7df-43af-40aa-8056-24a97b9c4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OME_1(sample_feature_arr, eps=10.0, l=20, m=4):\n",
    "    \n",
    "    float_bin_2 = lambda x: float_to_binary(x, m, l-m-1)\n",
    "    float_to_binary_vec_2 = np.vectorize(float_bin_2)\n",
    "    bin_float_2 = lambda x: binary_to_float(x, m, l-m-1)\n",
    "    binary_to_float_vec_2 = np.vectorize(bin_float_2)\n",
    "\n",
    "    r = sample_feature_arr.shape[1]\n",
    "    \n",
    "    float_to_binary_vec = np.vectorize(float_bin)\n",
    "    binary_to_float_vec = np.vectorize(bin_float)\n",
    "\n",
    "    feat_tmp = float_to_binary_vec_2(sample_feature_arr)\n",
    "    feat = np.apply_along_axis(string_to_int, axis=1, arr=feat_tmp)\n",
    "\n",
    "    rl = r * l\n",
    "    alpha_ome = 100\n",
    "    index_matrix_1 = np.array([alpha_ome / (1+ alpha_ome), 1/ (1+alpha_ome**3)]*int(l/2)) # np.array(range(l))\n",
    "    index_matrix_0 = np.array([ (alpha_ome * np.exp(eps/rl)) /(1 + alpha_ome* np.exp(eps/rl))]*int(l) )\n",
    "    p_1 = np.tile(index_matrix_1, (sample_feature_arr.shape[0], r))\n",
    "    p_0 = np.tile(index_matrix_0, (sample_feature_arr.shape[0], r))\n",
    "\n",
    "    p_temp = np.random.rand(p_0.shape[0], p_0.shape[1])\n",
    "    perturb_0 = (p_temp > p_0).astype(int)\n",
    "    perturb_1 = (p_temp > p_1).astype(int)\n",
    "\n",
    "    perturb_feat = np.array(torch.where(torch.tensor(feat)>0, torch.tensor((perturb_1 + feat)%2), torch.tensor((perturb_0 + feat)%2)) )\n",
    "    perturb_feat = np.apply_along_axis(join_string, axis=1, arr=perturb_feat)\n",
    "\n",
    "    perturb_feat = binary_to_float_vec_2(perturb_feat)\n",
    "    return torch.squeeze(torch.tensor(perturb_feat, dtype=torch.float))#.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff4ae7-a133-4a38-9588-81bf192d8ba5",
   "metadata": {},
   "source": [
    "# Set model's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca72b69-7b55-42be-be9d-97631fa1c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 2*n_inputs)\n",
    "        self.fc2 = nn.Linear(2*n_inputs, 1)\n",
    "        \n",
    "    def adv_weights(self, target, tau, delta = 0):\n",
    "        K = torch.eye(self.fc1.in_features)\n",
    "        K = torch.cat((K, -K), 0)\n",
    "        self.fc1.weight.data = K\n",
    "        self.fc1.bias.data = torch.cat((-target - delta, target - delta), 0)\n",
    "        \n",
    "        self.fc2.weight.data[0] = -torch.ones(self.fc2.in_features)\n",
    "        self.fc2.bias.data[0] = tau\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        fc2 = self.fc2(x)\n",
    "        x = F.relu(fc2)\n",
    "        return x, fc2\n",
    "    \n",
    "def task_tpr(i):\n",
    "    np.random.seed(int((time.time()+i*1000)))\n",
    "    x_test_threat = torch.cat((pattern.unsqueeze(0), x_test[np.random.randint(0, x_test.shape[0], D-1)]))\n",
    "#     x_test_threat = OME_1(x_test_threat, eps, l=l, m=m) # change this for OME\n",
    "    x_test_threat = BitRand_1(x_test_threat, eps, l=l, m=m)\n",
    "    return x_test_threat\n",
    "\n",
    "def task_tnr(i):\n",
    "    np.random.seed(int((time.time()+i*1000)))\n",
    "    x_test_threat = x_test[np.random.randint(0, x_test.shape[0], D)]\n",
    "#     x_test_threat = OME_1(x_test_threat, eps, l=l, m=m) # change this for OME\n",
    "    x_test_threat = BitRand_1(x_test_threat, eps, l=l, m=m)\n",
    "    return x_test_threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b11f38-e236-46b7-a83f-a19c89d6c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd533cb-2854-468d-afef-e803a95b11bc",
   "metadata": {},
   "source": [
    "# Find Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf740a9-14da-4403-8b2c-28e0f9006b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c82cc802-b028-4806-a580-0613918227f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=6 --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [04:01<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:56<00:00,  1.18s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:04<00:00, 41.25it/s]\n"
     ]
    }
   ],
   "source": [
    "D = 10*32 # batch size n \\times lx\n",
    "times = 200 # num of runs\n",
    "\n",
    "model = Classifier(x_test.shape[1], 2)\n",
    "model.adv_weights(pattern, 0.0)\n",
    "\n",
    "# eps_taus = np.arange(2,7.1,0.5) # results in the paper\n",
    "eps_taus = np.arange(6,7,1) # test for one run\n",
    "\n",
    "taus = []\n",
    "for eps_tau in eps_taus:\n",
    "    \n",
    "    eps = eps_tau\n",
    "    print(f'eps={eps} --------------')\n",
    "    \n",
    "    torch.set_num_threads(1) # Required for multiprocessing \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESS) as pool:\n",
    "        x_tpr = list(tqdm(pool.imap_unordered(task_tpr, range(times), chunksize=5), total=times))\n",
    "        x_tnr = list(tqdm(pool.imap_unordered(task_tnr, range(times), chunksize=5), total=times))\n",
    "\n",
    "    torch.set_num_threads(72) # Required for multiprocessing \n",
    "    \n",
    "    tmp_tau = 0\n",
    "    for x in tqdm(x_tpr):\n",
    "        out = model(x)[1]\n",
    "        out_pattern = out[0]\n",
    "        out_base = torch.mean(out[1:])\n",
    "        tmp_tau = tmp_tau + (out_pattern + out_base)/2 \n",
    "    tmp_tau = tmp_tau/len(x_tpr)\n",
    "    taus.append(tmp_tau)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57bc119-08d4-40cf-bc36-94c3fc98b1ab",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbdd45c-2de0-48fc-90ae-1a05c036a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "advs = []\n",
    "tprs = []\n",
    "tnrs = []\n",
    "\n",
    "runs = 1\n",
    "# runs = 10 # recommend number for good statistics \n",
    "games = 100\n",
    "times = runs*games\n",
    "\n",
    "# log_filename = 'API/imdb-res/IMDB_FC_OME_10.pkl'\n",
    "log_filename = 'API/imdb-res/IMDB_FC_BitRand_10.pkl' \n",
    "\n",
    "result_log = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "083cb197-fc37-4be5-92df-552146abee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=6 --------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:59<00:00,  1.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:56<00:00,  1.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 55.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpr = 1.0\n",
      "tnr = 1.0\n",
      "adv = 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(eps_taus)):\n",
    "    \n",
    "    eps = eps_taus[i]\n",
    "    print(f'eps={eps} --------------------------------')\n",
    "    model = Classifier(x_test.shape[1], 2)\n",
    "    model.adv_weights(pattern, -taus[i].detach())\n",
    "    \n",
    "    torch.set_num_threads(1) # Required for multiprocessing \n",
    "    \n",
    "    \n",
    "    with multiprocessing.Pool(processes=NUM_PROCESS) as pool:\n",
    "        x_tnr = list(tqdm(pool.imap_unordered(task_tnr, range(times), chunksize=2), total=times))\n",
    "        x_tpr = list(tqdm(pool.imap_unordered(task_tpr, range(times), chunksize=2), total=times))\n",
    "        \n",
    "    torch.set_num_threads(72) # Required for multiprocessing \n",
    "    \n",
    "    tpr = []\n",
    "    for x in tqdm(x_tpr):\n",
    "        out = model(x)[0]\n",
    "        if torch.sum(out > 0) > 0:\n",
    "            tpr.append(1)\n",
    "        else:\n",
    "            tpr.append(0)\n",
    "\n",
    "    tnr = []\n",
    "    for x in tqdm(x_tnr):\n",
    "        out = model(x)[0]\n",
    "        if torch.sum(out > 0) == 0:\n",
    "            tnr.append(1)\n",
    "        else:\n",
    "            tnr.append(0)\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start_index = i*games\n",
    "        end_index = (i+1)*games\n",
    "\n",
    "        tp = sum(tpr[start_index:end_index])/games\n",
    "        tn = sum(tnr[start_index:end_index])/games\n",
    "        adv = tp/2 + tn/2\n",
    "        \n",
    "        report = {'eps' : eps,\n",
    "                'adv': adv}\n",
    "        \n",
    "        result_log = pd.concat([result_log, pd.DataFrame.from_records([report])])\n",
    "        with open(log_filename, 'wb') as logfile:\n",
    "            pickle.dump(result_log, logfile)\n",
    "            \n",
    "    print(f'tpr = {np.mean(tpr)}')\n",
    "    print(f'tnr = {np.mean(tnr)}')\n",
    "    print(f'adv = {np.mean(tpr)/2 + np.mean(tnr)/2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1337e5e-7762-4f7b-a8ed-6c048e512845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
